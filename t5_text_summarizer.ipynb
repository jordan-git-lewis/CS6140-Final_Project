{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Using a T5 model for Text Summarization"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Install dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45216,"status":"ok","timestamp":1701131342410,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"xpEBdtcU1nl6","outputId":"f054c3b8-8873-4f19-fe6a-e2b900dd7d42"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package punkt to /home/jordan/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["!pip install datasets\n","!pip install transformers\n","!pip install pandas\n","!pip install accelerate -U\n","!pip install pip install transformers[torch]\n","!pip install stable-baselines --upgrade\n","!pip install rouge_score\n","!pip install numpy\n","!pip install textblob\n","!pip install -U scikit-learn scipy matplotlib\n","!pip install torch\n","\n","import datasets\n","from transformers import pipeline\n","import numpy as np\n","import re  # Used for data cleaning \n","import nltk  # Natural Language Toolkit\n","nltk.download('punkt')                                                    \n"]},{"cell_type":"markdown","metadata":{"id":"F6z_ObAAKzxV"},"source":["### 2. Import dataset from file\n","\n","This can work on a variety of datasets. This code assumes that the data is installed in a local directory. For our project we are using the Samsum Dataset Text Summarization set from Kaggle: https://www.kaggle.com/datasets/nileshmalode1/samsum-dataset-text-summarization/data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2708,"status":"ok","timestamp":1701127461294,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"FD28yJswff4R"},"outputs":[],"source":["from datasets import load_dataset\n","import pandas as pd\n","\n","train_file = '/home/data1/T5/CS6140_Final/datasets/Samsum/samsum-train.csv'\n","val_file = '/home/data1/T5/CS6140_Final/datasets/Samsum/samsum-validation.csv'\n","test_file = '/home/data1/T5/CS6140_Final/datasets/Samsum/samsum-test.csv'\n","\n","train_dataset = pd.read_csv(train_file)\n","val_dataset = pd.read_csv(val_file)\n","test_dataset = pd.read_csv(test_file)\n"]},{"cell_type":"markdown","metadata":{"id":"xx7eQfJUNoSj"},"source":["#### 2a. Display datasets \n","Referenced from: https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":217,"status":"ok","timestamp":1701127591511,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"rlQ2uvsBNhQL"},"outputs":[],"source":["def display_feature_list(features, feature_type):\n","\n","    '''\n","    This function displays the features within each list for each type of data\n","    '''\n","\n","    print(f\"\\n{feature_type} Features: \")\n","    print(', '.join(features) if features else 'None')\n","\n","def describe_df(df):\n","    \"\"\"\n","    This function prints some basic info on the dataset and\n","    sets global variables for feature lists.\n","    \"\"\"\n","\n","    global categorical_features, continuous_features, binary_features\n","    categorical_features = [col for col in df.columns if df[col].dtype == 'object']\n","    binary_features = [col for col in df.columns if df[col].nunique() <= 2 and df[col].dtype != 'object']\n","    continuous_features = [col for col in df.columns if df[col].dtype != 'object' and col not in binary_features]\n","\n","    print(f\"\\n{type(df).__name__} shape: {df.shape}\")\n","    print(f\"\\n{df.shape[0]:,.0f} samples\")\n","    print(f\"\\n{df.shape[1]:,.0f} attributes\")\n","    print(f'\\nMissing Data: \\n{df.isnull().sum()}')\n","    print(f'\\nDuplicates: {df.duplicated().sum()}')\n","    print(f'\\nData Types: \\n{df.dtypes}')\n","\n","    display_feature_list(categorical_features, 'Categorical')\n","    display_feature_list(continuous_features, 'Continuous')\n","    display_feature_list(binary_features, 'Binary')\n","\n","    print(f'\\n{type(df).__name__} Head: \\n')\n","    display(df.head(5))\n","    print(f'\\n{type(df).__name__} Tail: \\n')\n","    display(df.tail(5))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1701127593138,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"IVdN-XcF1lgG","outputId":"388e42d4-58b7-435c-8f64-a417c6f8ad11"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","DataFrame shape: (14731, 3)\n","\n","14,731 samples\n","\n","3 attributes\n","\n","Missing Data: \n","id          0\n","dialogue    0\n","summary     0\n","dtype: int64\n","\n","Duplicates: 0\n","\n","Data Types: \n","id          object\n","dialogue    object\n","summary     object\n","dtype: object\n","\n","Categorical Features: \n","id, dialogue, summary\n","\n","Continuous Features: \n","None\n","\n","Binary Features: \n","None\n","\n","DataFrame Head: \n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13818513</td>\n","      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n","      <td>Amanda baked cookies and will bring Jerry some...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13728867</td>\n","      <td>Olivia: Who are you voting for in this electio...</td>\n","      <td>Olivia and Olivier are voting for liberals in ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13681000</td>\n","      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n","      <td>Kim may try the pomodoro technique recommended...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>13730747</td>\n","      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n","      <td>Edward thinks he is in love with Bella. Rachel...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13728094</td>\n","      <td>Sam: hey  overheard rick say something\\r\\nSam:...</td>\n","      <td>Sam is confused, because he overheard Rick com...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           dialogue  \\\n","0  13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n","1  13728867  Olivia: Who are you voting for in this electio...   \n","2  13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n","3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n","4  13728094  Sam: hey  overheard rick say something\\r\\nSam:...   \n","\n","                                             summary  \n","0  Amanda baked cookies and will bring Jerry some...  \n","1  Olivia and Olivier are voting for liberals in ...  \n","2  Kim may try the pomodoro technique recommended...  \n","3  Edward thinks he is in love with Bella. Rachel...  \n","4  Sam is confused, because he overheard Rick com...  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","DataFrame Tail: \n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>14726</th>\n","      <td>13863028</td>\n","      <td>Romeo: You are on my ‘People you may know’ lis...</td>\n","      <td>Romeo is trying to get Greta to add him to her...</td>\n","    </tr>\n","    <tr>\n","      <th>14727</th>\n","      <td>13828570</td>\n","      <td>Theresa: &lt;file_photo&gt;\\r\\nTheresa: &lt;file_photo&gt;...</td>\n","      <td>Theresa is at work. She gets free food and fre...</td>\n","    </tr>\n","    <tr>\n","      <th>14728</th>\n","      <td>13819050</td>\n","      <td>John: Every day some bad news. Japan will hunt...</td>\n","      <td>Japan is going to hunt whales again. Island an...</td>\n","    </tr>\n","    <tr>\n","      <th>14729</th>\n","      <td>13828395</td>\n","      <td>Jennifer: Dear Celia! How are you doing?\\r\\nJe...</td>\n","      <td>Celia couldn't make it to the afternoon with t...</td>\n","    </tr>\n","    <tr>\n","      <th>14730</th>\n","      <td>13729017</td>\n","      <td>Georgia: are you ready for hotel hunting? We n...</td>\n","      <td>Georgia and Juliette are looking for a hotel i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                                           dialogue  \\\n","14726  13863028  Romeo: You are on my ‘People you may know’ lis...   \n","14727  13828570  Theresa: <file_photo>\\r\\nTheresa: <file_photo>...   \n","14728  13819050  John: Every day some bad news. Japan will hunt...   \n","14729  13828395  Jennifer: Dear Celia! How are you doing?\\r\\nJe...   \n","14730  13729017  Georgia: are you ready for hotel hunting? We n...   \n","\n","                                                 summary  \n","14726  Romeo is trying to get Greta to add him to her...  \n","14727  Theresa is at work. She gets free food and fre...  \n","14728  Japan is going to hunt whales again. Island an...  \n","14729  Celia couldn't make it to the afternoon with t...  \n","14730  Georgia and Juliette are looking for a hotel i...  "]},"metadata":{},"output_type":"display_data"}],"source":["describe_df(train_dataset)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1701127595542,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"h5bV-aDm4k0q","outputId":"0e6e502b-76b3-4e39-bfae-d5a214a20f30"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","DataFrame shape: (818, 3)\n","\n","818 samples\n","\n","3 attributes\n","\n","Missing Data: \n","id          0\n","dialogue    0\n","summary     0\n","dtype: int64\n","\n","Duplicates: 0\n","\n","Data Types: \n","id          object\n","dialogue    object\n","summary     object\n","dtype: object\n","\n","Categorical Features: \n","id, dialogue, summary\n","\n","Continuous Features: \n","None\n","\n","Binary Features: \n","None\n","\n","DataFrame Head: \n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13817023</td>\n","      <td>A: Hi Tom, are you busy tomorrow’s afternoon?\\...</td>\n","      <td>A will go to the animal shelter tomorrow to ge...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13716628</td>\n","      <td>Emma: I’ve just fallen in love with this adven...</td>\n","      <td>Emma and Rob love the advent calendar. Lauren ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13829420</td>\n","      <td>Jackie: Madison is pregnant\\r\\nJackie: but she...</td>\n","      <td>Madison is pregnant but she doesn't want to ta...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>13819648</td>\n","      <td>Marla: &lt;file_photo&gt;\\r\\nMarla: look what I foun...</td>\n","      <td>Marla found a pair of boxers under her bed.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13728448</td>\n","      <td>Robert: Hey give me the address of this music ...</td>\n","      <td>Robert wants Fred to send him the address of t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           dialogue  \\\n","0  13817023  A: Hi Tom, are you busy tomorrow’s afternoon?\\...   \n","1  13716628  Emma: I’ve just fallen in love with this adven...   \n","2  13829420  Jackie: Madison is pregnant\\r\\nJackie: but she...   \n","3  13819648  Marla: <file_photo>\\r\\nMarla: look what I foun...   \n","4  13728448  Robert: Hey give me the address of this music ...   \n","\n","                                             summary  \n","0  A will go to the animal shelter tomorrow to ge...  \n","1  Emma and Rob love the advent calendar. Lauren ...  \n","2  Madison is pregnant but she doesn't want to ta...  \n","3        Marla found a pair of boxers under her bed.  \n","4  Robert wants Fred to send him the address of t...  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","DataFrame Tail: \n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>813</th>\n","      <td>13829423</td>\n","      <td>Carla: I've got it...\\r\\nDiego: what?\\r\\nCarla...</td>\n","      <td>Carla's date for graduation is on June 4th. Di...</td>\n","    </tr>\n","    <tr>\n","      <th>814</th>\n","      <td>13727710</td>\n","      <td>Gita: Hello, this is Beti's Mum Gita, I wanted...</td>\n","      <td>Bev is going on the school trip with her son. ...</td>\n","    </tr>\n","    <tr>\n","      <th>815</th>\n","      <td>13829261</td>\n","      <td>Julia: Greg just texted me\\r\\nRobert: ugh, del...</td>\n","      <td>Greg cheated on Julia. He apologises to her. R...</td>\n","    </tr>\n","    <tr>\n","      <th>816</th>\n","      <td>13680226</td>\n","      <td>Marry: I broke my nail ;(\\r\\nTina: oh, no!\\r\\n...</td>\n","      <td>Marry broke her nail and has a party tomorrow....</td>\n","    </tr>\n","    <tr>\n","      <th>817</th>\n","      <td>13862383</td>\n","      <td>Paige: I asked them to wait and send the decla...</td>\n","      <td>Paige wants to have the declaration sent later...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                           dialogue  \\\n","813  13829423  Carla: I've got it...\\r\\nDiego: what?\\r\\nCarla...   \n","814  13727710  Gita: Hello, this is Beti's Mum Gita, I wanted...   \n","815  13829261  Julia: Greg just texted me\\r\\nRobert: ugh, del...   \n","816  13680226  Marry: I broke my nail ;(\\r\\nTina: oh, no!\\r\\n...   \n","817  13862383  Paige: I asked them to wait and send the decla...   \n","\n","                                               summary  \n","813  Carla's date for graduation is on June 4th. Di...  \n","814  Bev is going on the school trip with her son. ...  \n","815  Greg cheated on Julia. He apologises to her. R...  \n","816  Marry broke her nail and has a party tomorrow....  \n","817  Paige wants to have the declaration sent later...  "]},"metadata":{},"output_type":"display_data"}],"source":["describe_df(val_dataset)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1701127596379,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"cJ_CSg3L4mdk","outputId":"38702eb1-5352-4a26-e8f5-10b2b77fb0fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","DataFrame shape: (819, 3)\n","\n","819 samples\n","\n","3 attributes\n","\n","Missing Data: \n","id          0\n","dialogue    0\n","summary     0\n","dtype: int64\n","\n","Duplicates: 0\n","\n","Data Types: \n","id          object\n","dialogue    object\n","summary     object\n","dtype: object\n","\n","Categorical Features: \n","id, dialogue, summary\n","\n","Continuous Features: \n","None\n","\n","Binary Features: \n","None\n","\n","DataFrame Head: \n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13862856</td>\n","      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n","      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13729565</td>\n","      <td>Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...</td>\n","      <td>Eric and Rob are going to watch a stand-up on ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13680171</td>\n","      <td>Lenny: Babe, can you help me with something?\\r...</td>\n","      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>13729438</td>\n","      <td>Will: hey babe, what do you want for dinner to...</td>\n","      <td>Emma will be home soon and she will let Will k...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13828600</td>\n","      <td>Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...</td>\n","      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           dialogue  \\\n","0  13862856  Hannah: Hey, do you have Betty's number?\\nAman...   \n","1  13729565  Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...   \n","2  13680171  Lenny: Babe, can you help me with something?\\r...   \n","3  13729438  Will: hey babe, what do you want for dinner to...   \n","4  13828600  Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...   \n","\n","                                             summary  \n","0  Hannah needs Betty's number but Amanda doesn't...  \n","1  Eric and Rob are going to watch a stand-up on ...  \n","2  Lenny can't decide which trousers to buy. Bob ...  \n","3  Emma will be home soon and she will let Will k...  \n","4  Jane is in Warsaw. Ollie and Jane has a party....  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","DataFrame Tail: \n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>814</th>\n","      <td>13611902-1</td>\n","      <td>Alex: Were you able to attend Friday night's b...</td>\n","      <td>Benjamin didn't come to see a basketball game ...</td>\n","    </tr>\n","    <tr>\n","      <th>815</th>\n","      <td>13820989</td>\n","      <td>Jamilla: remember that the audition starts at ...</td>\n","      <td>The audition starts at 7.30 P.M. in Antena 3.</td>\n","    </tr>\n","    <tr>\n","      <th>816</th>\n","      <td>13717193</td>\n","      <td>Marta: &lt;file_gif&gt;\\r\\nMarta: Sorry girls, I cli...</td>\n","      <td>Marta sent a file accidentally,</td>\n","    </tr>\n","    <tr>\n","      <th>817</th>\n","      <td>13829115</td>\n","      <td>Cora: Have you heard how much fuss British med...</td>\n","      <td>There was a meet-and-greet with James Charles ...</td>\n","    </tr>\n","    <tr>\n","      <th>818</th>\n","      <td>13818810</td>\n","      <td>Rachel: &lt;file_other&gt;\\r\\nRachel: Top 50 Best Fi...</td>\n","      <td>Rachel sends a list of Top 50 films of 2018. J...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                                           dialogue  \\\n","814  13611902-1  Alex: Were you able to attend Friday night's b...   \n","815    13820989  Jamilla: remember that the audition starts at ...   \n","816    13717193  Marta: <file_gif>\\r\\nMarta: Sorry girls, I cli...   \n","817    13829115  Cora: Have you heard how much fuss British med...   \n","818    13818810  Rachel: <file_other>\\r\\nRachel: Top 50 Best Fi...   \n","\n","                                               summary  \n","814  Benjamin didn't come to see a basketball game ...  \n","815      The audition starts at 7.30 P.M. in Antena 3.  \n","816                    Marta sent a file accidentally,  \n","817  There was a meet-and-greet with James Charles ...  \n","818  Rachel sends a list of Top 50 films of 2018. J...  "]},"metadata":{},"output_type":"display_data"}],"source":["describe_df(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"XfUmUcIuQkB6"},"source":["### 3. Preprocessing Data\n","We can see in the data that there are characters representing files, or photos like ```<file_photo>```. These will need to be cleaned. In addition, there are null values within the datasets that can be removed"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1701127597841,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"Ze84Sjl0l4a-"},"outputs":[],"source":["# The ID label is not necessary for this.\n","categorical_features.remove('id')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1701127598878,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"AHblNWHLLHbr"},"outputs":[],"source":["# Remove null values\n","train_dataset = train_dataset.dropna()\n","val_dataset = val_dataset.dropna()\n","test_dataset = test_dataset.dropna()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701127599973,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"XzsbIs4CQihO"},"outputs":[],"source":["def clean_tags(text):\n","    clean = re.compile('<.*?>') # Compiling tags\n","    clean = re.sub(clean, '', text) # Replacing tags text by an empty string\n","\n","    # Removing empty dialogues\n","    clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n","\n","    return clean"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701127600661,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"gxZ_rJcJc7Tv"},"outputs":[],"source":["def clean_dataframe(df, column_labels):\n","    for col in column_labels:\n","        df[col] = df[col].fillna('').apply(clean_tags)\n","    return df"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1701127601475,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"XBTl0yZfc_H5"},"outputs":[],"source":["train_dataset = clean_dataframe(train_dataset,['dialogue', 'summary'])\n","test_dataset = clean_dataframe(test_dataset,['dialogue', 'summary'])\n","val_dataset = clean_dataframe(val_dataset,['dialogue', 'summary'])"]},{"cell_type":"markdown","metadata":{"id":"UJW_36wqKt3u"},"source":["### Print Sample"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1701127613822,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"QTEdzJ4XnF-o","outputId":"1830b18b-5224-4f7b-c45d-08567095c4ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Steve: Can you remind me what pages we were supposed to read for tomorrow's classes?\n","Chris: You mean the International Political Relations?\n","Steve: Yup.\n","Chris: pages 10-25\n","Steve: Thanks a lot man.\n","Steve: By the way, do you think we really need to read that?\n","Chris: Yup. The guys passionate about these texts he makes students read. He gets angry when he sees people don't read it.\n","Chris: When he's not satisfied, he often punishes students with quicktests...\n","Steve: I hate the educational system, when everybody tries to discourage me from absorbing the knowledge...\n","Chris: Nothing comes easy.\n","Steve: So they say... \n"]}],"source":["print(train_dataset[\"dialogue\"].iloc[13950])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701127614607,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"f6Eq1Al8muWk"},"outputs":[],"source":["# Converts pandas dataframe to a Dataset\n","train_dataset = datasets.Dataset.from_pandas(train_dataset)\n","test_dataset = datasets.Dataset.from_pandas(test_dataset)\n","val_dataset = datasets.Dataset.from_pandas(val_dataset)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1701128019737,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"zvyPuSM_IYXi","outputId":"12e4b775-6d51-4517-984c-d957bc83ae53"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'dialogue', 'summary'],\n","    num_rows: 14731\n","})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Verify the data structure\n","train_dataset"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Tokenize the data\n","For this we will use the t5-base model.\n","Tokenizer script referenced from: https://medium.com/askdata/train-t5-for-text-summarization-a1926f52d281"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268,"referenced_widgets":["65b8909f6a764b89b28bbc499138d188","84ca0a839e554fb388ddaf1c21ee7731","67d57a5024ac4e8284415056f00d40b4","d210934437494dab86d7fc4b8b3d5d46","8414158c47a345e684b6eb2580a19ab3","929e71bf721d454f9ecf5935daae14c1","b2915aa2b62d477d94c691a44fb29594","d2f193e4c1c540f0b24c9a065ac5cb92","1e2bda27f0e148f393a99b6407776920","7755224f7f1042d5a2be2e1da836bd8f","bc627a8165ec40e0b84e60b48c0907f0","4926df9478d84514a1e64ffde1e81f95","d348c4c7bbff455197fa8a068add5643","863b04976be146fbaa993ae0e74035c0","313f18af2720471d8e11e85c3ed1d443","3ac2327c534d422fab88fc489ef5bdf0","6b3139373b4f427f96d7764705cfa9e1","b6279f1bdc094ba980c55c4c46908b76","f0b36af905024478a7178f8b30a56110","0ca49a2f9ea04631b2e9f3e66347b3d6","1d6bdd70e8904427a206c1ff9fec72a2","1fbe5b0cf13a4292b081f92f46d5484f"]},"executionInfo":{"elapsed":12201,"status":"ok","timestamp":1701129528498,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"QCf2zHEz4oFN","outputId":"9d9c116d-f452-49da-8680-1ec080c39b27"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","Map:   0%|          | 0/14731 [00:00<?, ? examples/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","Map: 100%|██████████| 14731/14731 [00:02<00:00, 5600.67 examples/s]\n","Map: 100%|██████████| 818/818 [00:00<00:00, 6097.95 examples/s]\n","Map: 100%|██████████| 819/819 [00:00<00:00, 6127.87 examples/s]\n"]}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('t5-base')\n","\n","max_source = 1000\n","max_target = 128\n","\n","def tokenize(batch):\n","    \n","    tokenized_input = tokenizer(batch['dialogue'], max_length=max_source, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        tokenized_labels = tokenizer(batch[\"summary\"], max_length=128, truncation=True)\n","\n","    tokenized_input['labels'] = tokenized_labels['input_ids']\n","\n","    return tokenized_input\n","\n","train_dataset_tokenized = train_dataset.map(tokenize, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n","val_dataset_tokenized = val_dataset.map(tokenize, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n","test_dataset_tokenized = test_dataset.map(tokenize, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n","\n","train_dataset_tokenized.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n","val_dataset_tokenized.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n","test_dataset_tokenized.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"]},{"cell_type":"markdown","metadata":{},"source":["Verify Dataset dictionaries and column labels are correct"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701129945259,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"Y2eyoIbt-rvC","outputId":"6046694e-42de-4790-b208-a8ac3c1ee54b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 14731\n","})\n","Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 818\n","})\n","Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 819\n","})\n"]}],"source":["print(train_dataset_tokenized)\n","print(val_dataset_tokenized)\n","print(test_dataset_tokenized)"]},{"cell_type":"markdown","metadata":{"id":"zxpuX6uxS_ea"},"source":["## 5. Computing Metrics\n","To evaluate our model we will be using the ROUGE score. This can be imported from the datasets library and input as an argument in our trainer.\n","Code referenced from: https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":515,"status":"ok","timestamp":1701130789309,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"Tz7epVp4SOBJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_2061369/280211341.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = datasets.load_metric('rouge')\n"]}],"source":["metric = datasets.load_metric('rouge')"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_prediction):\n","    \n","    # Retrieve prediction and label\n","    predictions, labels = eval_prediction\n","    \n","    # Decoding predictions\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    \n","    # Obtaining the true labels tokens, while eliminating masked tokens\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    \n","    \n","    # Computing rouge score\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} \n","\n","    # Add mean-generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":2470,"status":"ok","timestamp":1701130831722,"user":{"displayName":"Jordan Lewis","userId":"02851842510840619583"},"user_tz":480},"id":"BCKt6ma-RSlu"},"outputs":[],"source":["# Instantiating Data Collator - required for seq2seq trainer\n","from transformers import T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","model = T5ForConditionalGeneration.from_pretrained('t5-small')\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU\n"]}],"source":["# Enable GPU power if available\n","import torch\n","\n","if torch.cuda.is_available():\n","    print(\"Using GPU\")\n","    device = torch.device('cuda')\n","else:\n","    print(\"Using CPU\")\n","    device = torch.device('cpu')\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Run Trainer\n","For training we utilize the seq2seq trainer from huggingface: https://huggingface.co/docs/transformers/main_classes/trainer\n","\n","Fine tuning the hyperparameters was done by running multiple models for low epoch values"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"ykI8G26ODZOh","outputId":"c1b8c065-7e4d-466a-b62f-477ccd45d70a"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/3070 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 10%|█         | 307/3070 [03:24<29:49,  1.54it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                  \n"," 10%|█         | 307/3070 [03:42<29:49,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.8259080648422241, 'eval_rouge1': 39.826, 'eval_rouge2': 16.2435, 'eval_rougeL': 32.6084, 'eval_rougeLsum': 36.6319, 'eval_gen_len': 16.8217, 'eval_runtime': 18.6659, 'eval_samples_per_second': 43.877, 'eval_steps_per_second': 1.875, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 16%|█▋        | 500/3070 [05:52<28:41,  1.49it/s]  "]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.0582, 'learning_rate': 8.371335504885994e-05, 'epoch': 1.63}\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 614/3070 [07:06<27:30,  1.49it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                  \n"," 20%|██        | 614/3070 [07:24<27:30,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.7647408246994019, 'eval_rouge1': 40.6385, 'eval_rouge2': 17.768, 'eval_rougeL': 33.7577, 'eval_rougeLsum': 37.5597, 'eval_gen_len': 16.8291, 'eval_runtime': 18.1169, 'eval_samples_per_second': 45.206, 'eval_steps_per_second': 1.932, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 30%|███       | 921/3070 [10:49<23:40,  1.51it/s]  /home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                  \n"," 30%|███       | 921/3070 [11:07<23:40,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.7425086498260498, 'eval_rouge1': 41.5634, 'eval_rouge2': 18.6244, 'eval_rougeL': 34.6747, 'eval_rougeLsum': 38.4635, 'eval_gen_len': 16.7021, 'eval_runtime': 18.0925, 'eval_samples_per_second': 45.267, 'eval_steps_per_second': 1.934, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 33%|███▎      | 1000/3070 [11:58<22:06,  1.56it/s] "]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.8789, 'learning_rate': 6.742671009771987e-05, 'epoch': 3.26}\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 1228/3070 [14:30<23:38,  1.30it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n"," 40%|████      | 1228/3070 [14:48<23:38,  1.30it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.7244532108306885, 'eval_rouge1': 41.6636, 'eval_rouge2': 18.5605, 'eval_rougeL': 34.8702, 'eval_rougeLsum': 38.5296, 'eval_gen_len': 16.8022, 'eval_runtime': 17.8542, 'eval_samples_per_second': 45.872, 'eval_steps_per_second': 1.96, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 49%|████▉     | 1500/3070 [17:48<18:54,  1.38it/s]  "]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.8017, 'learning_rate': 5.114006514657981e-05, 'epoch': 4.89}\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 1535/3070 [18:12<17:58,  1.42it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n"," 50%|█████     | 1535/3070 [18:30<17:58,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.713733434677124, 'eval_rouge1': 42.1674, 'eval_rouge2': 18.8684, 'eval_rougeL': 35.3144, 'eval_rougeLsum': 38.8547, 'eval_gen_len': 16.8449, 'eval_runtime': 18.0718, 'eval_samples_per_second': 45.319, 'eval_steps_per_second': 1.937, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 60%|██████    | 1842/3070 [21:53<13:00,  1.57it/s]  /home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n"," 60%|██████    | 1842/3070 [22:11<13:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.704947590827942, 'eval_rouge1': 42.3811, 'eval_rouge2': 19.0467, 'eval_rougeL': 35.2594, 'eval_rougeLsum': 39.0351, 'eval_gen_len': 16.9573, 'eval_runtime': 18.1806, 'eval_samples_per_second': 45.048, 'eval_steps_per_second': 1.925, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 65%|██████▌   | 2000/3070 [23:57<12:44,  1.40it/s]  "]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.7563, 'learning_rate': 3.485342019543974e-05, 'epoch': 6.51}\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 2149/3070 [25:36<09:54,  1.55it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n"," 70%|███████   | 2149/3070 [25:54<09:54,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.7039817571640015, 'eval_rouge1': 42.7403, 'eval_rouge2': 19.2299, 'eval_rougeL': 35.5139, 'eval_rougeLsum': 39.3526, 'eval_gen_len': 16.8999, 'eval_runtime': 18.2717, 'eval_samples_per_second': 44.824, 'eval_steps_per_second': 1.916, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 80%|████████  | 2456/3070 [29:18<06:07,  1.67it/s]  /home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n"," 80%|████████  | 2456/3070 [29:36<06:07,  1.67it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.695377230644226, 'eval_rouge1': 43.0632, 'eval_rouge2': 19.4124, 'eval_rougeL': 35.6752, 'eval_rougeLsum': 39.5463, 'eval_gen_len': 16.8645, 'eval_runtime': 18.1417, 'eval_samples_per_second': 45.145, 'eval_steps_per_second': 1.929, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 81%|████████▏ | 2500/3070 [30:06<06:39,  1.43it/s]  "]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.7305, 'learning_rate': 1.8566775244299675e-05, 'epoch': 8.14}\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 2763/3070 [33:00<03:25,  1.49it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n"," 90%|█████████ | 2763/3070 [33:17<03:25,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.6927355527877808, 'eval_rouge1': 42.9158, 'eval_rouge2': 19.3969, 'eval_rougeL': 35.5905, 'eval_rougeLsum': 39.4768, 'eval_gen_len': 16.9084, 'eval_runtime': 17.8049, 'eval_samples_per_second': 45.999, 'eval_steps_per_second': 1.966, 'epoch': 9.0}\n"]},{"name":"stderr","output_type":"stream","text":["/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"," 98%|█████████▊| 3000/3070 [35:55<00:44,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.7104, 'learning_rate': 2.280130293159609e-06, 'epoch': 9.77}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3070/3070 [36:41<00:00,  1.63it/s]/home/data1/T5/CS6140_Final/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","                                                   \n","100%|██████████| 3070/3070 [36:59<00:00,  1.63it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.6928337812423706, 'eval_rouge1': 43.0563, 'eval_rouge2': 19.4298, 'eval_rougeL': 35.7404, 'eval_rougeLsum': 39.6116, 'eval_gen_len': 16.9267, 'eval_runtime': 18.1568, 'eval_samples_per_second': 45.107, 'eval_steps_per_second': 1.928, 'epoch': 10.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3070/3070 [37:00<00:00,  1.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 2220.0479, 'train_samples_per_second': 66.354, 'train_steps_per_second': 1.383, 'train_loss': 1.8200511248958227, 'epoch': 10.0}\n"]}],"source":["output_dir = '/home/data1/T5/CS6140_Final/out/runs/'\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir = output_dir,\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = 'epoch',\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps=2,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    num_train_epochs=10,\n","    predict_with_generate=True, # whether to use generate to calculate generative metrics (ROUGE score)\n","    fp16=False,\n",")\n","\n","# Defining Trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset_tokenized,\n","    eval_dataset=test_dataset_tokenized,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","trainer.save_model(output_dir + '/model')"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Inference\n","Although we get updated score results during the training process, it is important to evaluate the summaries produced by our model on the validation set. Even if our model summary does not exactly match the dataset summary, the ability to edit the length of returned summary shows promising results based on human analysis.  "]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["model = '/home/data1/T5/CS6140_Final/out/runs/model'"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def generate_summary(validation_dataset, model, index_position, summary_length):\n","    \n","    text = validation_dataset[index_position]['dialogue']\n","    summary = validation_dataset[index_position]['summary']\n","    summarizer = pipeline('summarization', model = model)\n","    generated_summary = summarizer(text, max_length=summary_length)\n","    \n","    print('Original Dialogue:\\n')\n","    print(text)\n","    print('\\n')\n","    print('Dataset Summary:\\n')\n","    print(summary)\n","    print('\\n')\n","    print('Model-generated Summary:\\n')\n","    print(generated_summary)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Dialogue:\n","\n","John: doing anything special?\n","Alex: watching 'Millionaires' on tvn\n","Sam: me too! He has a chance to win a million!\n","John: ok, fingers crossed then! :)\n","\n","\n","Reference Summary:\n","\n","Alex and Sam are watching Millionaires.\n","\n","\n","Model-generated Summary:\n","\n","[{'summary_text': \"Alex is watching 'Millionaires' on tv. John and Sam are going to win a million if he wins\"}]\n"]}],"source":["generate_summary(val_dataset, model, 35, 30)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Dialogue:\n","\n","A: Hi Tom, are you busy tomorrow’s afternoon?\n","B: I’m pretty sure I am. What’s up?\n","A: Can you go with me to the animal shelter?.\n","B: What do you want to do?\n","A: I want to get a puppy for my son.\n","B: That will make him so happy.\n","A: Yeah, we’ve discussed it many times. I think he’s ready now.\n","B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n","A: I'll get him one of those little dogs.\n","B: One that won't grow up too big;-)\n","A: And eat too much;-))\n","B: Do you know which one he would like?\n","A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n","B: I bet you had to drag him away.\n","A: He wanted to take it home right away ;-).\n","B: I wonder what he'll name it.\n","A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n","\n","\n","Reference Summary:\n","\n","A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n","\n","\n","Model-generated Summary:\n","\n","[{'summary_text': 'A wants to get a puppy for her son. A took him to the animal shelter last Monday and he wanted to drag him away.'}]\n"]}],"source":["generate_summary(val_dataset, model, 0, 30)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Dialogue:\n","\n","Mike: Do you know where Tomas is from?\n","Jenny: Eastern Europe I believe\n","Mike: sure, but what country exactly\n","Mike: I heard him speaking English today with Kamil, so I think he's not Polish\n","Jack: Really? I was sure he was Polish\n","Kyle: He's from Slovenia\n","Mike: oh, how cute, how do you know?\n","Kyle: We talked many times about Slovenia and his home town\n","Mike: Which is?\n","Kyle: Bled I think, close to the Alps\n","Jack: and why do you find Slovenia cute? hahaha\n","Mike: I think he's the only Slovenian in the company now\n","Jack: true, quite exotic\n","\n","\n","Reference Summary:\n","\n","Mike, Jenny and Jack wonder where Tomas is from. Kyle is sure Tomas is from Slovenia. Mike thinks Tomas is now the only Slovenian in the company.\n","\n","\n","Model-generated Summary:\n","\n","[{'summary_text': \"Tomas is from Eastern Europe, but he's not Polish. He's from Slovenia and is the only Slovenian in the company now.\"}]\n"]}],"source":["generate_summary(val_dataset, model, 100, 45)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"1obZjUuVwks3A4oFX3gXFtPOM2LtrPQfL","timestamp":1699558159986}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ca49a2f9ea04631b2e9f3e66347b3d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d6bdd70e8904427a206c1ff9fec72a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2bda27f0e148f393a99b6407776920":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1fbe5b0cf13a4292b081f92f46d5484f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"313f18af2720471d8e11e85c3ed1d443":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d6bdd70e8904427a206c1ff9fec72a2","placeholder":"​","style":"IPY_MODEL_1fbe5b0cf13a4292b081f92f46d5484f","value":" 818/818 [00:00&lt;00:00, 2725.95 examples/s]"}},"3ac2327c534d422fab88fc489ef5bdf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4926df9478d84514a1e64ffde1e81f95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d348c4c7bbff455197fa8a068add5643","IPY_MODEL_863b04976be146fbaa993ae0e74035c0","IPY_MODEL_313f18af2720471d8e11e85c3ed1d443"],"layout":"IPY_MODEL_3ac2327c534d422fab88fc489ef5bdf0"}},"65b8909f6a764b89b28bbc499138d188":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84ca0a839e554fb388ddaf1c21ee7731","IPY_MODEL_67d57a5024ac4e8284415056f00d40b4","IPY_MODEL_d210934437494dab86d7fc4b8b3d5d46"],"layout":"IPY_MODEL_8414158c47a345e684b6eb2580a19ab3"}},"67d57a5024ac4e8284415056f00d40b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2f193e4c1c540f0b24c9a065ac5cb92","max":14731,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e2bda27f0e148f393a99b6407776920","value":14731}},"6b3139373b4f427f96d7764705cfa9e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7755224f7f1042d5a2be2e1da836bd8f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8414158c47a345e684b6eb2580a19ab3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84ca0a839e554fb388ddaf1c21ee7731":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_929e71bf721d454f9ecf5935daae14c1","placeholder":"​","style":"IPY_MODEL_b2915aa2b62d477d94c691a44fb29594","value":"Map: 100%"}},"863b04976be146fbaa993ae0e74035c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0b36af905024478a7178f8b30a56110","max":818,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ca49a2f9ea04631b2e9f3e66347b3d6","value":818}},"929e71bf721d454f9ecf5935daae14c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2915aa2b62d477d94c691a44fb29594":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6279f1bdc094ba980c55c4c46908b76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc627a8165ec40e0b84e60b48c0907f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d210934437494dab86d7fc4b8b3d5d46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7755224f7f1042d5a2be2e1da836bd8f","placeholder":"​","style":"IPY_MODEL_bc627a8165ec40e0b84e60b48c0907f0","value":" 14731/14731 [00:10&lt;00:00, 2276.63 examples/s]"}},"d2f193e4c1c540f0b24c9a065ac5cb92":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d348c4c7bbff455197fa8a068add5643":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b3139373b4f427f96d7764705cfa9e1","placeholder":"​","style":"IPY_MODEL_b6279f1bdc094ba980c55c4c46908b76","value":"Map: 100%"}},"f0b36af905024478a7178f8b30a56110":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
